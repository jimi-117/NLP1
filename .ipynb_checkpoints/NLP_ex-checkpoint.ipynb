{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering for NLP in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ce notebook s'appuit sur un cours en Anglais de Datacamp . Vous pouvez utiliser un traducteur.\n",
    "Voici le lien pour accéder aux vidéos et aux slides :\n",
    "https://www.dropbox.com/sh/jinhd7xyjq8gmxi/AAAXcjnfKGIB5gRBE3iI_3JPa?dl=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programme\n",
    "\n",
    "Chapitre 1 - Basic features and readability scores\n",
    "\n",
    "    Learn to compute basic features such as number of words, number of characters, average word length and number of special characters\n",
    "    (such as Twitter hashtags and mentions). \n",
    "    You will also learn to compute readability scores and determine the amount of education required to comprehend a piece of text.\n",
    "\n",
    "\n",
    "Chapitre 2 - Text preprocessing, POS tagging and NER\n",
    "   \n",
    "    In this chapter, you will learn about tokenization and lemmatization. You will then learn how to perform text cleaning, part-of-speech tagging, and named entity recognition using the spaCy library. Upon mastering these concepts,\n",
    "    you will proceed to make the Gettysburg address machine-friendly, analyze noun usage in fake news, and identify people mentioned in a TechCrunch article.\n",
    "\n",
    "\n",
    "Chapitre 3 - N-Gram models\n",
    "\n",
    "    Learn about n-gram modeling and use it to perform sentiment analysis on movie reviews.\n",
    "\n",
    "\n",
    "Chapitre 4 -TF-IDF and similarity scores\n",
    "\n",
    "    Learn how to compute tf-idf weights and the cosine similarity score between two vectors. You will use these concepts to build a movie and a TED Talk recommender. Finally, you will also learn about word embeddings and using word vector representations, you will compute similarities between various Pink Floyd songs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 1 - Basic features and readability scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir la première vidéo du chapitre 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv( \"df1.csv\", sep =';').dropna(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature 1</th>\n",
       "      <th>feature 2</th>\n",
       "      <th>feature 3</th>\n",
       "      <th>feature 4 feature 5</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>48.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>63.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77.9583</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>39.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>53.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>51.4792</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49.5042</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>47.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>227.5250</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>227.5250</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>24.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.3000</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>26.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78.8500</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>80.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature 1  feature 2  feature 3  feature 4 feature 5   label\n",
       "0     29.0000          0          0             211.3375  female\n",
       "1      0.9167          1          2             151.5500    male\n",
       "2      2.0000          1          2             151.5500  female\n",
       "3     30.0000          1          2             151.5500    male\n",
       "4     25.0000          1          2             151.5500  female\n",
       "5     48.0000          0          0              26.5500    male\n",
       "6     63.0000          1          0              77.9583  female\n",
       "7     39.0000          0          0               0.0000    male\n",
       "8     53.0000          2          0              51.4792  female\n",
       "9     71.0000          0          0              49.5042    male\n",
       "10    47.0000          1          0             227.5250    male\n",
       "11    18.0000          1          0             227.5250  female\n",
       "12    24.0000          0          0              69.3000  female\n",
       "13    26.0000          0          0              78.8500  female\n",
       "14    80.0000          0          0              30.0000    male"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#afficher les caractéristique de df1 (ie les colonnes)\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df1 contient des caractéristiques catégorielles et n'est donc pas adapté à l'application d'algorithmes de ML.\n",
    "\n",
    "Dans cet exercice, votre tâche consiste à convertir df1 dans un format adapté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utiliser pd.get_dummies() pour modifier les caractéristiques catégorielles\n",
    "\n",
    "____\n",
    "\n",
    "#afficher le nouveau dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature 1</th>\n",
       "      <th>feature 2</th>\n",
       "      <th>feature 3</th>\n",
       "      <th>feature 4 feature 5</th>\n",
       "      <th>label_female</th>\n",
       "      <th>label_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature 1  feature 2  feature 3  feature 4 feature 5  label_female  \\\n",
       "0    29.0000          0          0             211.3375             1   \n",
       "1     0.9167          1          2             151.5500             0   \n",
       "2     2.0000          1          2             151.5500             1   \n",
       "3    30.0000          1          2             151.5500             0   \n",
       "4    25.0000          1          2             151.5500             1   \n",
       "\n",
       "   label_male  \n",
       "0           0  \n",
       "1           1  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir la 2eme vidéo du chapitre 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer le csv russian_tweets\n",
    "\n",
    "tweets = pd.read_csv( \"russian_tweets.csv\", sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127447</td>\n",
       "      <td>LIVE STREAM VIDEO=&gt; Donald Trump Rallies in Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123642</td>\n",
       "      <td>Muslim Attacks NYPD Cops with Meat Cleaver. Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226970</td>\n",
       "      <td>.@vfpatlas well that's a swella word there (di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138339</td>\n",
       "      <td>RT wehking_pamela: Bobby_Axelrod2k MMFlint don...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161610</td>\n",
       "      <td>Жители обстреливаемых районов Донецка проводят...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>24329</td>\n",
       "      <td>How To Inspire People With Your Music! https:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>315060</td>\n",
       "      <td>... https://t.co/AfWdTkKQlm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>361901</td>\n",
       "      <td>Trevor Noah: Until we start treating racism li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>119948</td>\n",
       "      <td>SenSanders: RT SenJeffMerkley: We must act bol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>242678</td>\n",
       "      <td>Police: Man arrested for February shooting at ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                            content\n",
       "0        127447  LIVE STREAM VIDEO=> Donald Trump Rallies in Co...\n",
       "1        123642  Muslim Attacks NYPD Cops with Meat Cleaver. Me...\n",
       "2        226970  .@vfpatlas well that's a swella word there (di...\n",
       "3        138339  RT wehking_pamela: Bobby_Axelrod2k MMFlint don...\n",
       "4        161610  Жители обстреливаемых районов Донецка проводят...\n",
       "..          ...                                                ...\n",
       "995       24329  How To Inspire People With Your Music! https:/...\n",
       "996      315060                        ... https://t.co/AfWdTkKQlm\n",
       "997      361901  Trevor Noah: Until we start treating racism li...\n",
       "998      119948  SenSanders: RT SenJeffMerkley: We must act bol...\n",
       "999      242678  Police: Man arrested for February shooting at ...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans cet exercice, vous avez un dataframe qui contient des tweets associés à l'Agence russe de recherche Internet.\n",
    "\n",
    "Votre tâche est de créer une nouvelle feature «char_count» qui calcule le nombre de caractères pour chaque tweet. Calculez également la longueur moyenne de chaque tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.462\n"
     ]
    }
   ],
   "source": [
    "# Créer une nouvelle feature  char_count\n",
    "____\n",
    "\n",
    "# Printer la longueur moyenne des tweets\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We're going to talk — my — a new lecture, just...</td>\n",
       "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a representation of your brain, and yo...</td>\n",
       "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a great honor today to share with you The...</td>\n",
       "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My passions are music, technology and making t...</td>\n",
       "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It used to be that if you wanted to get a comp...</td>\n",
       "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  We're going to talk — my — a new lecture, just...   \n",
       "1  This is a representation of your brain, and yo...   \n",
       "2  It's a great honor today to share with you The...   \n",
       "3  My passions are music, technology and making t...   \n",
       "4  It used to be that if you wanted to get a comp...   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
       "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
       "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
       "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
       "4  https://www.ted.com/talks/jeremy_howard_the_wo...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importer le csv ted\n",
    "ted = pd.read_csv( \"ted.csv\", sep =',')\n",
    "\n",
    "ted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ted est un dataframe qui contient les transcriptions de 500 conférences TED. Votre travail consiste à calculer une nouvelle fonctionnalité word_count qui contient le nombre approximatif de mots pour chaque discours. Par conséquent, vous devez également calculer le nombre moyen de mots des exposés. Les transcriptions sont disponibles dans la colonne transcript.\n",
    "\n",
    "Vous devrez définir une fonction count_words qui prend une chaîne de charactères comme argument et renvoie le nombre de mots de la chaîne. Vous devrez ensuite appliquer cette fonction à la colonne de transcript de ted pour créer pour créer une nouvelle feature (caractéristique). \n",
    "\n",
    "Il faut ensuite printer le  nombre moyen de mots de toutes les transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1987.1\n"
     ]
    }
   ],
   "source": [
    "# définisser la fonction count_words\n",
    "\n",
    "def count_words(string):\n",
    "\t# Split the string into words\n",
    "    ____\n",
    "    \n",
    "    # Return the number of words\n",
    "    ____\n",
    "\n",
    "# créer une nouvelle colonne contenant le nombre de mots de chaque transcription\n",
    "____\n",
    "\n",
    "# Calculer le nombre moyen de mots\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revenons aux tweets russes. Dans cet exercice, vous calculerez le nombre de hashtags et de mentions @ dans chaque tweet en définissant respectivement deux fonctions count_hashtags () et count_mentions () et en les appliquant à la fonction de contenu des tweets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Définisser la fonction qui compte les hashtags. \n",
    "#utilisez startswith () pour vérifier si un mot particulier commence par '#'.\n",
    "\n",
    "def count_hashtags(string):\n",
    "\t# Split string en  words\n",
    "    ____\n",
    "    \n",
    "    # créer une liste qui contient les mots avec #\n",
    "    ____\n",
    "    \n",
    "    # Retoruner le nombre hashtags\n",
    "    ____\n",
    "\n",
    "# Créer une nouvelle feature contenant le nombre de hashtags dans 'content'\n",
    "____\n",
    "\n",
    "#afficher un histogramme des du nombre de hashtags\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# définisser une fonction qui compte le nombre de mentions @mention\n",
    "#utilisez startswith () pour vérifier si un mot particulier commence par '@'.\n",
    "\n",
    "def count_mentions(string):\n",
    "\t# Split string en  words\n",
    "    ____\n",
    "    \n",
    "     # créer une liste qui contient les mots avec @\n",
    "    ____\n",
    "    \n",
    "    # Retoruner le nombre de mentions\n",
    "    ____\n",
    "\n",
    "#afficher un histogramme des du nombre de hashtags\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 2 - Text preprocessing, POS tagging and NER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir vidéo 1 du chapitre 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Identifiez la liste des mots parmi les choix qui n'ont pas le même lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 He, She, I, They \n",
    "\n",
    "2 Am, Are, Is, Was\n",
    "\n",
    "3 Increase, Increases, Increasing, Increased\n",
    "\n",
    "4 Car, Bike, Truck, Bus\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans cet exercice, vous \"tokeniserer\" l'un des discours les plus célèbres de tous les temps: le discours de Gettysburg prononcé par le président américain Abraham Lincoln pendant la guerre civile américaine.\n",
    "\n",
    "Le discours entier est disponible sous la forme d'une chaîne nommée gettysburg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "gettysburg = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we're engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We're met on a great battlefield of that war. We've come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It's altogether fitting and proper that we should do this. But, in a larger sense, we can't dedicate - we can not consecrate - we can not hallow - this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It's rather for us to be here dedicated to the great task remaining before us - that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion - that we here highly resolve that these dead shall not have died in vain - that this nation, under God, shall have a new birth of freedom - and that government of the people, by the people, for the people, shall not perish from the earth.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Four', 'score', 'and', 'seven', 'years', 'ago', 'our', 'fathers', 'brought', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceived', 'in', 'Liberty', ',', 'and', 'dedicated', 'to', 'the', 'proposition', 'that', 'all', 'men', 'are', 'created', 'equal', '.', 'Now', 'we', \"'re\", 'engaged', 'in', 'a', 'great', 'civil', 'war', ',', 'testing', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceived', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', 'We', \"'re\", 'met', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', 'We', \"'ve\", 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'gave', 'their', 'lives', 'that', 'that', 'nation', 'might', 'live', '.', 'It', \"'s\", 'altogether', 'fitting', 'and', 'proper', 'that', 'we', 'should', 'do', 'this', '.', 'But', ',', 'in', 'a', 'larger', 'sense', ',', 'we', 'ca', \"n't\", 'dedicate', '-', 'we', 'can', 'not', 'consecrate', '-', 'we', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'The', 'brave', 'men', ',', 'living', 'and', 'dead', ',', 'who', 'struggled', 'here', ',', 'have', 'consecrated', 'it', ',', 'far', 'above', 'our', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'The', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', 'we', 'say', 'here', ',', 'but', 'it', 'can', 'never', 'forget', 'what', 'they', 'did', 'here', '.', 'It', 'is', 'for', 'us', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicated', 'here', 'to', 'the', 'unfinished', 'work', 'which', 'they', 'who', 'fought', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', 'It', \"'s\", 'rather', 'for', 'us', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remaining', 'before', 'us', '-', 'that', 'from', 'these', 'honored', 'dead', 'we', 'take', 'increased', 'devotion', 'to', 'that', 'cause', 'for', 'which', 'they', 'gave', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', 'we', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'died', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# charger 'en_core_web_sm'\n",
    "____\n",
    "\n",
    "# Créer un Doc object\n",
    "____)\n",
    "\n",
    "# Generer les tokens et les printer\n",
    "____\n",
    "\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['four', 'score', 'and', 'seven', 'year', 'ago', '-PRON-', 'father', 'bring', 'forth', 'on', 'this', 'continent', ',', 'a', 'new', 'nation', ',', 'conceive', 'in', 'Liberty', ',', 'and', 'dedicate', 'to', 'the', 'proposition', 'that', 'all', 'man', 'be', 'create', 'equal', '.', 'now', '-PRON-', 'be', 'engage', 'in', 'a', 'great', 'civil', 'war', ',', 'test', 'whether', 'that', 'nation', ',', 'or', 'any', 'nation', 'so', 'conceive', 'and', 'so', 'dedicated', ',', 'can', 'long', 'endure', '.', '-PRON-', 'be', 'meet', 'on', 'a', 'great', 'battlefield', 'of', 'that', 'war', '.', '-PRON-', 'have', 'come', 'to', 'dedicate', 'a', 'portion', 'of', 'that', 'field', ',', 'as', 'a', 'final', 'resting', 'place', 'for', 'those', 'who', 'here', 'give', '-PRON-', 'life', 'that', 'that', 'nation', 'may', 'live', '.', '-PRON-', 'be', 'altogether', 'fitting', 'and', 'proper', 'that', '-PRON-', 'should', 'do', 'this', '.', 'but', ',', 'in', 'a', 'large', 'sense', ',', '-PRON-', 'can', 'not', 'dedicate', '-', '-PRON-', 'can', 'not', 'consecrate', '-', '-PRON-', 'can', 'not', 'hallow', '-', 'this', 'ground', '.', 'the', 'brave', 'man', ',', 'living', 'and', 'dead', ',', 'who', 'struggle', 'here', ',', 'have', 'consecrate', '-PRON-', ',', 'far', 'above', '-PRON-', 'poor', 'power', 'to', 'add', 'or', 'detract', '.', 'the', 'world', 'will', 'little', 'note', ',', 'nor', 'long', 'remember', 'what', '-PRON-', 'say', 'here', ',', 'but', '-PRON-', 'can', 'never', 'forget', 'what', '-PRON-', 'do', 'here', '.', '-PRON-', 'be', 'for', '-PRON-', 'the', 'living', ',', 'rather', ',', 'to', 'be', 'dedicate', 'here', 'to', 'the', 'unfinished', 'work', 'which', '-PRON-', 'who', 'fight', 'here', 'have', 'thus', 'far', 'so', 'nobly', 'advanced', '.', '-PRON-', 'be', 'rather', 'for', '-PRON-', 'to', 'be', 'here', 'dedicated', 'to', 'the', 'great', 'task', 'remain', 'before', '-PRON-', '-', 'that', 'from', 'these', 'honor', 'dead', '-PRON-', 'take', 'increase', 'devotion', 'to', 'that', 'cause', 'for', 'which', '-PRON-', 'give', 'the', 'last', 'full', 'measure', 'of', 'devotion', '-', 'that', '-PRON-', 'here', 'highly', 'resolve', 'that', 'these', 'dead', 'shall', 'not', 'have', 'die', 'in', 'vain', '-', 'that', 'this', 'nation', ',', 'under', 'God', ',', 'shall', 'have', 'a', 'new', 'birth', 'of', 'freedom', '-', 'and', 'that', 'government', 'of', 'the', 'people', ',', 'by', 'the', 'people', ',', 'for', 'the', 'people', ',', 'shall', 'not', 'perish', 'from', 'the', 'earth', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# charger 'en_core_web_sm'\n",
    "____\n",
    "\n",
    "# Créer un Doc object\n",
    "____\n",
    "\n",
    "# Generer les lemmas\n",
    "____\n",
    "\n",
    "print( ____)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "four score and seven year ago -PRON- father bring forth on this continent , a new nation , conceive in Liberty , and dedicate to the proposition that all man be create equal . now -PRON- be engage in a great civil war , test whether that nation , or any nation so conceive and so dedicated , can long endure . -PRON- be meet on a great battlefield of that war . -PRON- have come to dedicate a portion of that field , as a final resting place for those who here give -PRON- life that that nation may live . -PRON- be altogether fitting and proper that -PRON- should do this . but , in a large sense , -PRON- can not dedicate - -PRON- can not consecrate - -PRON- can not hallow - this ground . the brave man , living and dead , who struggle here , have consecrate -PRON- , far above -PRON- poor power to add or detract . the world will little note , nor long remember what -PRON- say here , but -PRON- can never forget what -PRON- do here . -PRON- be for -PRON- the living , rather , to be dedicate here to the unfinished work which -PRON- who fight here have thus far so nobly advanced . -PRON- be rather for -PRON- to be here dedicated to the great task remain before -PRON- - that from these honor dead -PRON- take increase devotion to that cause for which -PRON- give the last full measure of devotion - that -PRON- here highly resolve that these dead shall not have die in vain - that this nation , under God , shall have a new birth of freedom - and that government of the people , by the people , for the people , shall not perish from the earth .\n"
     ]
    }
   ],
   "source": [
    "# convertir lemmas en string (utiliser .join() ) et les printer\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir la 2eme vidéo du chapitre 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exercice, on va traiter l'extrait d'un article de blog. Votre tâche consiste à préparer ce texte dans un format plus convivial pour la machine. Cela impliquera la conversion en minuscules, la lemmatisation et la suppression des mots vides, des ponctuations et des caractères non alphabétiques.\n",
    "\n",
    "L'extrait est disponible dans blog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog = '\\nTwenty-first-century politics has witnessed an alarming rise of populism in the U.S. and Europe. The first warning signs came with the UK Brexit Referendum vote in 2016 swinging in the way of Leave. This was followed by a stupendous victory by billionaire Donald Trump to become the 45th President of the United States in November 2016. Since then, Europe has seen a steady rise in populist and far-right parties that have capitalized on Europe’s Immigration Crisis to raise nationalist and anti-Europe sentiments. Some instances include Alternative for Germany (AfD) winning 12.6% of all seats and entering the Bundestag, thus upsetting Germany’s political order for the first time since the Second World War, the success of the Five Star Movement in Italy and the surge in popularity of neo-nazism and neo-fascism in countries such as Hungary, Czech Republic, Poland and Austria.\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "century politic witness alarm rise populism Europe warning sign come UK Brexit Referendum vote swinge way Leave follow stupendous victory billionaire Donald Trump President United States November Europe steady rise populist far right party capitalize Europe Immigration Crisis raise nationalist anti europe sentiment instance include Alternative Germany AfD win seat enter Bundestag upset Germany political order time Second World War success Star Movement Italy surge popularity neo nazism neo fascism country Hungary Czech Republic Poland Austria\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle et créer un  object doc\n",
    "____\n",
    "\n",
    "# obetnir une liste de stopwords\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Generer des tokens lemmatizés\n",
    "____\n",
    "\n",
    "# Enlever les stopwords et les termes non alphabétiques (utiliser .isalpha()) tokens\n",
    "____\n",
    "\n",
    "# convertir lemmas en string et les printer\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dans cet exercice, nous reprennons les TED Talks du premier chapitre. Votre tâche consiste à nettoyer ces exposés à l'aide des techniques décrites précédemment en écrivant une fonction et en l'appliquant à la colonne \"transcript\" du dataframe df .\n",
    "\n",
    "La liste des mots vides est disponible sous forme de mots vides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      talk new lecture TED illusion create TED try r...\n",
      "1      representation brain brain break left half log...\n",
      "2      great honor today share Digital Universe creat...\n",
      "3      passion music technology thing combination thi...\n",
      "4      use want computer new program programming requ...\n",
      "                             ...                        \n",
      "495    today unpack example iconic design perfect sen...\n",
      "496    brother belong demographic Pat percent accord ...\n",
      "497    John Hockenberry great Tom want start question...\n",
      "498    right moment kill More car internet little mob...\n",
      "499    real problem math education right basically ha...\n",
      "Name: transcript_preprocessed, Length: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Définisser une fonction pour traiter le texte\n",
    "def preprocess(text):\n",
    "    # créer 1 Doc object\n",
    "    ____\n",
    "    # Générer les lemmas\n",
    "    ____\n",
    "    # Enlever les  stopwords and non-alphabetiques éléments\n",
    "    ____\n",
    "    \n",
    "    return ____\n",
    "  \n",
    "# appliquer ce preprocess à ted['transcript']\n",
    "ted['transcript_preprocessed'] = ____\n",
    "print(ted['transcript_preprocessed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparer ted['transcript_preprocessed'] et ted['transcript']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir vidéo 3 du chapitre 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exercice, vous effectuerez un \"part-of-speech tagging\" d'une partie du discours sur un passage célèbre de l'un des romans les plus connus de tous les temps, Lord of the Flies, écrit par William Golding.\n",
    "\n",
    "Le passage est disponible dans lotf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lotf = 'He found himself understanding the wearisomeness of this life, where every path was an improvisation and a considerable part of one’s waking life was spent watching one’s feet.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRON'), ('found', 'VERB'), ('himself', 'PRON'), ('understanding', 'VERB'), ('the', 'DET'), ('wearisomeness', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('life', 'NOUN'), (',', 'PUNCT'), ('where', 'ADV'), ('every', 'DET'), ('path', 'NOUN'), ('was', 'AUX'), ('an', 'DET'), ('improvisation', 'NOUN'), ('and', 'CCONJ'), ('a', 'DET'), ('considerable', 'ADJ'), ('part', 'NOUN'), ('of', 'ADP'), ('one', 'NOUN'), ('’s', 'PART'), ('waking', 'VERB'), ('life', 'NOUN'), ('was', 'AUX'), ('spent', 'VERB'), ('watching', 'VERB'), ('one', 'PRON'), ('’s', 'PART'), ('feet', 'NOUN'), ('.', 'PUNCT')]\n"
     ]
    }
   ],
   "source": [
    "# charger  en_core_web_sm model\n",
    "____\n",
    "\n",
    "# créer un Doc object\n",
    "____\n",
    "\n",
    "# Générer tokens et pos tags\n",
    "____\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans cet exercice, nous écrirons deux fonctions, les nums () et les proper_nouns () qui compteront respectivement le nombre de noms et de noms propres dans un morceau de texte.\n",
    "\n",
    "Ces fonctions prendront un morceau de texte et généreront une liste contenant les balises POS pour chaque mot. Elles renverront alors le nombre de noms propres / autres noms que contient le texte. Nous utiliserons ces fonctions dans le prochain exercice pour générer des informations intéressantes sur les fake news.\n",
    "\n",
    "vous pouvez tester la fonction sur la phrase suivante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"Abdul, Bill and Cathy went to the market to buy apples.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# retourne le nombre de noms propres \n",
    "def proper_nouns(text, model=nlp):\n",
    "  \t# créer un doc object\n",
    "    ____\n",
    "    # générer une liste de POS tags\n",
    "    ____\n",
    "    \n",
    "    # retourne le nombre de noms propres\n",
    "    return ____\n",
    "\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# retourne le nombre de noms  \n",
    "def nouns(text, model=nlp):\n",
    "  \t# créer un doc object\n",
    "    ____\n",
    "    # générer une liste de POS tags\n",
    "    ____\n",
    "    \n",
    "    # retourne le nombre de noms\n",
    "    return ____\n",
    "\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exercice, vous avez reçu dans un dataframe des titres d'actualité fausses ou réelles. Votre tâche consiste à générer deux nouvelles features num_propn et num_noun qui représentent le nombre de noms propres et d'autres noms contenus dans la colonne titre.\n",
    "\n",
    "Ensuite, nous calculerons le nombre moyen de noms propres et des noms utilisés dans les faux et vrais titres des actualités et comparerons les valeurs. S'il y a une différence notoire, il y a de fortes chances que l'utilisation des features num_propn et num_noun dans des détecteurs de fake news améliorerait leur performances.\n",
    "\n",
    "Pour accomplir cette tâche, vous utiliserez les fonctions proper_nouns et nouns que vous aviez construits dans l'exercice précédent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title label\n",
       "0           0                       You Can Smell Hillary’s Fear  FAKE\n",
       "1           1  Watch The Exact Moment Paul Ryan Committed Pol...  FAKE\n",
       "2           2        Kerry to go to Paris in gesture of sympathy  REAL\n",
       "3           3  Bernie supporters on Twitter erupt in anger ag...  FAKE\n",
       "4           4   The Battle of New York: Why This Primary Matters  REAL"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on charge les données\n",
    "\n",
    "headlines = pd.read_csv( \"fakenews.csv\", sep =',').dropna(axis=1)\n",
    "\n",
    "headlines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no. of proper nouns in real and fake headlines are 2.42 and 4.58 respectively\n"
     ]
    }
   ],
   "source": [
    "#appliquer la fonction proper_nouns à la colonne title et stocker le resultat dans une nouvelle colonne headlines['num_propn']\n",
    "____\n",
    "\n",
    "# Calculer les moyennes de noms propres dans les fake et real news\n",
    "____\n",
    "\n",
    "# Afficher le resultat \n",
    "print(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean no. of other nouns in real and fake headlines are 2.30 and 1.67 respectively\n"
     ]
    }
   ],
   "source": [
    "#appliquer la fonction proper_nouns à la colonne title et stocker le resultat dans une nouvelle colonne headlines['num_noun']\n",
    "____\n",
    "\n",
    "# Calculer les moyennes de noms propres dans les fake et real news\n",
    "____\n",
    "\n",
    "# Afficher le resultat \n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir Vidéo 4 chapitre 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans cet exercice, nous identifierons et classerons les étiquettes de diverses entités nommées dans un corps de texte en utilisant l'un des modèles statistiques de spaCy. Nous vérifierons également la véracité de ces étiquettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sundar Pichai PERSON\n",
      "Google ORG\n",
      "Mountain View GPE\n"
     ]
    }
   ],
   "source": [
    "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
    "\n",
    "# Charger le modèle 'en_core_web_sm'\n",
    "____\n",
    "\n",
    "# Créer un doc objet contenant texte \n",
    "____\n",
    "\n",
    "# à l'aide d'une boucle afficher toutes les entités\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans cet exercice, vous avez reçu un extrait d'un article de presse publié dans TechCrunch. Votre tâche est d'écrire une fonction find_people qui identifie les noms des personnes qui ont été mentionnées dans un morceau de texte particulier. Vous utiliserez ensuite find_people pour identifier les personnes qui vous intéressent dans l'article.\n",
    "\n",
    "L'article est disponible dans tc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voici l'article\n",
    "tc = \"\\nIt’s' been a busy day for Facebook  exec op-eds. Earlier this morning, Sheryl Sandberg broke the site’s silence around the Christchurch massacre, and now Mark Zuckerberg is calling on governments and other bodies to increase regulation around the sorts of data Facebook traffics in. He’s hoping to get out in front of heavy-handed regulation and get a seat at the table shaping it.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sheryl Sandberg', 'Mark Zuckerberg']\n"
     ]
    }
   ],
   "source": [
    "#définir la fonction find_persons\n",
    "\n",
    "def find_persons(text):\n",
    "  # Créer un doc objet\n",
    "  ____\n",
    "  \n",
    "  # identifier les personnes\n",
    "  ____\n",
    "  \n",
    "  # renvoyer les personnes identifiées\n",
    "  return ____\n",
    "\n",
    "#appliquer la fonction à l'article tc\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapitre 3 - N-Gram models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir la vidéo 1 du chapitre 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On vous a donné un corpus de documents et vous avez calculé le vocabulaire du corpus comme suit: \n",
    "V: a, un, et, mais, peut, venir, soir, pour toujours, aller, moi, hommes, peut, sur, les femmes\n",
    "\n",
    "Lequel des énoncés suivants correspond au vecteur sac de mots (BOW) pour le texte «les hommes peuvent venir et les hommes peuvent partir mais je continue pour toujours»?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1 : (0, 0, 1, 1, 0, 1, 0, 1, 2, 1, 2, 2, 1, 0, 0)\n",
    "\n",
    "\n",
    "2 : (0, 1, 0, 1, 1, 1, 2, 0, 2, 1, 0, 0, 0, 2, 0)\n",
    "\n",
    "\n",
    "3 : (2, 1, 0, 0, 2, 1, 0, 0, 0, 1)\n",
    "\n",
    "\n",
    "4 : (0, 0, 1, 2, 1, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans cet exercice, vous avez reçu un corpus de plus de 7 000 slogans de films. Votre travail consiste à générer le sac de mots bow_matrix pour ces slogans. Pour cet exercice, nous ignorerons l'étape de prétraitement du texte et générerons directement bow_matrix.\n",
    "\n",
    "Nous étudierons également la forme de la bow_matrix résultante. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1            Roll the dice and unleash the excitement!\n",
       "2    Still Yelling. Still Fighting. Still Ready for...\n",
       "3    Friends are the people who let you be yourself...\n",
       "4    Just When His World Is Back To Normal... He's ...\n",
       "5                             A Los Angeles Crime Saga\n",
       "Name: tagline, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#on charge les slogans\n",
    "\n",
    "movies = pd.read_csv( \"movie_overviews.csv\", sep =',')\n",
    "\n",
    "corpus = movies[\"tagline\"].dropna()\n",
    "\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7033, 6614)\n"
     ]
    }
   ],
   "source": [
    "# Importer CountVectorizer\n",
    "____\n",
    "\n",
    "# Creer 1 CountVectorizer object\n",
    "____\n",
    "\n",
    "# Generer \"matrix of word vectors\" sur lem_corpus\n",
    "____\n",
    "\n",
    "# afficher la taille de bow_matrix\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La taille de la matrice est grande. Le corpus n'a pas été preprocessé. Il contient des majuscules, des stopwords et des vides.\n",
    "Vous allez maintenant prépocesser le corpus avec la fonction preprocess vu plus haut et calculer à nouveau la \"bow_matrix\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "créer lem_corpus contenant les versions prétraitées des slogans des films de l'exercice précédent. \n",
    "\n",
    "utiliser preprocess\n",
    ". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    roll dice unleash excitement\n",
       "2           yell fight ready Love\n",
       "3    friend people let let forget\n",
       "4      world normal Surprise life\n",
       "5          Los Angeles Crime Saga\n",
       "Name: tagline, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Appliquer votre fonction preprocess vu plus haut pour tout mettre en minuscule, lemmatisés et supprimer les mots vides stopwords.\n",
    "\n",
    "lem_corpus = ____\n",
    "\n",
    "lem_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Votre travail est maintenat de générer le sac de représentation de mots bow_lem_matrix pour ces slogans lemmatisés et de comparer sa forme avec celle de bow_matrix obtenue dans l'exercice précédent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7033, 5219)\n"
     ]
    }
   ],
   "source": [
    "# Importer CountVectorizer\n",
    "____\n",
    "\n",
    "# Creer 1 CountVectorizer object\n",
    "____\n",
    "\n",
    "# Generer \"matrix of word vectors\" sur lem_corpus\n",
    "____\n",
    "\n",
    "# afficher la taille de bow_matrix\n",
    "print(____.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que peut-on remarquer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans la vidéo de la leçon, vous avez vu que CountVectorizer n'indexait pas nécessairement le vocabulaire par ordre alphabétique. Dans cet exercice, nous allons apprendre à mapper chaque nom de colonne avec le mot du vocabulaire correspondant pour créer une bow_matrix de type dataframe.\n",
    "\n",
    "Nous utiliserons les trois phrases sur les lions de la vidéo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['The lion is the king of the jungle',\n",
    " 'Lions have lifespans of a decade',\n",
    " 'The lion is an endangered species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   an  decade  endangered  have  is  jungle  king  lifespans  lion  lions  of  \\\n",
      "0   0       0           0     0   1       1     1          0     1      0   1   \n",
      "1   0       1           0     1   0       0     0          1     0      1   1   \n",
      "2   1       0           1     0   1       0     0          0     1      0   0   \n",
      "\n",
      "   species  the  \n",
      "0        0    3  \n",
      "1        0    0  \n",
      "2        1    1  \n"
     ]
    }
   ],
   "source": [
    "#Instancier un objet CountVectorizer. Nommez-le vectorizer.\n",
    "\n",
    "____\n",
    "\n",
    "#En utilisant fit_transform (), générer bow_matrix pour le corpus.\n",
    "____\n",
    "\n",
    "#En utilisant la méthode get_feature_names (), mapper les noms des colonnes avec le mot correspondant dans le vocabulaire.\n",
    "____\n",
    "\n",
    "# afficher bow_df\n",
    "____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir la vidéo 2 du chapitre 3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dans cet exercice, vous allez manipuler deux séries pandas, X_train et X_test, qui consistent en des critiques de films.  Votre tâche consiste à prétraiter les revues et à générer des vecteurs BoW pour ces deux ensembles à l'aide de CountVectorizer.\n",
    "\n",
    "Une fois que nous aurons généré les matrices vectorielles BoW X_train_bow et X_test_bow, nous serons en très bonne position pour lui appliquer un modèle d'apprentissage automatique et effectuer une analyse des sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construction de la base de modélisation pour l'analyse des sentiments des revues de film\n",
    "\n",
    "reviews = pd.read_csv( \"movie_reviews_clean.csv\", sep =',')\n",
    "\n",
    "X =reviews['review']\n",
    "\n",
    "y = reviews['sentiment']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split(X,y, test_size=0.33, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 14403)\n",
      "(330, 14403)\n"
     ]
    }
   ],
   "source": [
    "#Importer CountVectorizer depuis la bibliothèque sklearn.\n",
    "____\n",
    "\n",
    "#Instancier un objet CountVectorizer nommé vectorizer. Assurez-vous que tous les mots sont convertis en minuscules et que les mots vides anglais sont supprimés.\n",
    "____\n",
    "\n",
    "#En utilisant X_train, ajuster vectorizer puis utiliser-le pour transformer X_train afin de générer l'ensemble des vecteurs BoW X_train_bow.\n",
    "____\n",
    "\n",
    "#transformez X_test en utilisant vectorizer pour générer l'ensemble des vecteurs BoW X_test_bow.\n",
    "____\n",
    "\n",
    "# Printer les tailles de X_train_bow et de X_test_bow\n",
    "print(____)\n",
    "print(____)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'exercice précédent, vous avez généré les représentations de sac de mots pour les données de revues de film d'entraînement et de test. Dans cet exercice, nous utiliserons ces données pour entrainer un classificateur Naive Bayes capable de détecter le sentiment d'une critique de film et de calculer sa précision. Notez que puisqu'il s'agit d'un problème de classification binaire, le modèle ne peut classer un avis que comme positif (1) ou négatif (0). Il est incapable de détecter les avis neutres.\n",
    "\n",
    "Comme vu plus haut, les vecteurs BoW d'entraînement et de test sont respectivement disponibles sous la forme X_train_bow et X_test_bow. Les libellés correspondants sont disponibles respectivement dans y_train et y_test. En outre, pour votre référence, le jeu de données de revue de film original est disponible dans le df reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.779\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "#importer MultinomialNB depuis sklearn.naive_bayes\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#instancier un objet de MultinomialNB. Nommez-le clf.\n",
    "____\n",
    "\n",
    "\n",
    "#Ajuster clf en utilisant X_train_bow et y_train.\n",
    "____\n",
    "\n",
    "#Mesurer la précision de clf en utilisant X_test_bow et y_test.\n",
    "accuracy = ____\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
    "\n",
    "# Predire le sentiment d'une revue négative\n",
    "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
    "prediction = ____\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir la vidéo 2 du chapitre 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exercice, nous allons manipuler plus de 9 000 slogans de films. Notre travail consiste à générer des modèles de n grammes n égal à 1, n égal à 2 et n égal à 3 pour ces données et découvrir le nombre de caractéristiques pour chaque modèle.\n",
    "\n",
    "Nous comparerons ensuite le nombre de \"features\" générées pour chaque modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    roll dice unleash excitement\n",
       "2           yell fight ready Love\n",
       "3    friend people let let forget\n",
       "4      world normal Surprise life\n",
       "5          Los Angeles Crime Saga\n",
       "Name: tagline, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Les slogans sont dans lem_corpus\n",
    "\n",
    "lem_corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ng1, ng2 and ng3 have 13, 27 and 39 features respectively\n"
     ]
    }
   ],
   "source": [
    "#Générer un modèle de n-grammes avec n = 1. Nommez-le ng1\n",
    "____\n",
    "\n",
    "#Générer un modèle n-gramme avec n = 1 et 2. Nommez-le ng2\n",
    "____\n",
    "\n",
    "#Générer un modèle n-gramme avec n = 1,2 et 3. Nommez-le ng3\n",
    "____\n",
    "\n",
    "#printer le nombre de \"features\" pour chaque modèle.\n",
    "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A l'image de l'exercice précédent, nous allons créer un classificateur qui peut détecter si la critique d'un film particulier est positive ou négative. Cependant, cette fois, nous utiliserons un n-grammes avec  n = 1 et 2 .\n",
    "\n",
    "Les revues de formation n-gram sont disponibles sous X_train_ng. Les revues de test correspondantes sont disponibles sous X_test_ng. Enfin, utilisez y_train et y_test pour accéder libellé, c'est à dire les sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(670, 79542)\n",
      "(330, 79542)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Instancier un objet CountVectorizer nommé vectorizer. Assurez-vous que tous les mots sont convertis en minuscules et que les mots vides anglais sont supprimés.\n",
    "____\n",
    "\n",
    "#En utilisant X_train, ajustez vectorizer puis utilisez-le pour transformer X_train afin de générer l'ensemble des vecteurs BoW X_train_bow.\n",
    "X_train_bow_ng = ____\n",
    "\n",
    "#transformer X_test en utilisant vectorizer pour générer l'ensemble des vecteurs BoW X_test_bow.\n",
    "X_test_bow_ng = ____\n",
    "\n",
    "# Printer les tailles de X_train_bow et de X_test_bow\n",
    "print(X_train_bow_ng.shape)\n",
    "print(X_test_bow_ng.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'exercice précédent, vous avez généré les représentations de sac de mots pour les données de revue de film d'entraînement et de test. Dans cet exercice, nous utiliserons ce données pour entrainer un classificateur Naive Bayes capable de détecter le sentiment d'une critique de film et de calculer sa précision. Notez que puisqu'il s'agit d'un problème de classification binaire, le modèle n'est capable de classer un avis que comme positif (1) ou négatif (0). Il est incapable de détecter les avis neutres.\n",
    "\n",
    "Dans le cas où vous ne vous en souvenez pas, les vecteurs d'entraînement et de test BoW sont respectivement disponibles sous la forme X_train_bow et X_test_bow. Les libellés correspondants sont disponibles respectivement comme y_train et y_test. En outre, pour votre référence, le jeu de données de revue de film original est disponible en tant que df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the classifier on the test set is 0.776\n",
      "The sentiment predicted by the classifier is 0\n"
     ]
    }
   ],
   "source": [
    "# Definir une instance  MultinomialNB \n",
    "clf_ng = MultinomialNB()\n",
    "\n",
    "# entrainer le modèle\n",
    "____\n",
    "\n",
    "# Mesurer la performance\n",
    "accuracy = ____\n",
    "print(\"The accuracy of the classifier on the test set is %.3f\" % ____)\n",
    "\n",
    "# Predict the sentiment of a negative review\n",
    "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
    "prediction = ____\n",
    "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous savez maintenant comment effectuer une analyse des sentiments en convertissant du texte en différentes représentations n-gram et en les alimentant dans un classificateur. Dans cet exercice, nous effectuerons une analyse des sentiments pour les mêmes critiques de films avant d'utiliser deux modèles de n grammes: unigramme et n-gramme jusqu'à n égal à 3.\n",
    "\n",
    "Nous comparerons ensuite les performances à l'aide de trois critères: la précision du modèle sur l'ensemble de test, le temps nécessaire à l'exécution du programme et le nombre de caractéristiques créées lors de la génération de la représentation n-gramme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 0.238 seconds to complete. The accuracy on the test set is 0.75. The ngram representation had 12347 features.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#modéliser avec un modèle unigrammes \n",
    "\n",
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(reviews['review'], reviews['sentiment'], test_size=0.5, random_state=42, stratify=reviews['sentiment'])\n",
    "\n",
    "# Generer le ngrams\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "\n",
    "#transformer train_X et text_X\n",
    "train_X = vectorizer.fit_transform(____)\n",
    "test_X = vectorizer.transform(____)\n",
    "\n",
    "# Entrainer le modèle\n",
    "clf = MultinomialNB()\n",
    "____\n",
    "\n",
    "# Predire le sentiment d'une revue négative\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ré-utiliser le code ci-dessus en utilisant des ngram allant jusqu'à 3. Que peut ont dire des performances du modèle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The program took 1.226 seconds to complete. The accuracy on the test set is 0.77. The ngram representation had 178240 features.\n"
     ]
    }
   ],
   "source": [
    "#modéliser avec n-gramme avec n allant jusqu'à  3\n",
    "\n",
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "train_X, test_X, train_y, test_y = train_test_split(reviews['review'], reviews['sentiment'], test_size=0.5, random_state=42, stratify=reviews['sentiment'])\n",
    "\n",
    "# Generer le ngrams\n",
    "____\n",
    "\n",
    "#transformer train_X et text_X\n",
    "\n",
    "____\n",
    "\n",
    "\n",
    "# Entrainer le modèle\n",
    "____\n",
    "\n",
    "# Predire le sentiment d'une revue négative\n",
    "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapitre 4 -TF-IDF and similarity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir la vidéo 1 du chapitre 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans cet exercice, on va utiliser ted qui contient les transcriptions de 500 conférences TED. Votre tâche est de générer les vecteurs tf-idf pour ces discussions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "      <th>url</th>\n",
       "      <th>word_count</th>\n",
       "      <th>transcript_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We're going to talk — my — a new lecture, just...</td>\n",
       "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
       "      <td>1704</td>\n",
       "      <td>talk new lecture TED illusion create TED try r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a representation of your brain, and yo...</td>\n",
       "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
       "      <td>1387</td>\n",
       "      <td>representation brain brain break left half log...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's a great honor today to share with you The...</td>\n",
       "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
       "      <td>890</td>\n",
       "      <td>great honor today share Digital Universe creat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My passions are music, technology and making t...</td>\n",
       "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
       "      <td>1548</td>\n",
       "      <td>passion music technology thing combination thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It used to be that if you wanted to get a comp...</td>\n",
       "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
       "      <td>3480</td>\n",
       "      <td>use want computer new program programming requ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          transcript  \\\n",
       "0  We're going to talk — my — a new lecture, just...   \n",
       "1  This is a representation of your brain, and yo...   \n",
       "2  It's a great honor today to share with you The...   \n",
       "3  My passions are music, technology and making t...   \n",
       "4  It used to be that if you wanted to get a comp...   \n",
       "\n",
       "                                                 url  word_count  \\\n",
       "0  https://www.ted.com/talks/al_seckel_says_our_b...        1704   \n",
       "1  https://www.ted.com/talks/aaron_o_connell_maki...        1387   \n",
       "2  https://www.ted.com/talks/carter_emmart_demos_...         890   \n",
       "3  https://www.ted.com/talks/jared_ficklin_new_wa...        1548   \n",
       "4  https://www.ted.com/talks/jeremy_howard_the_wo...        3480   \n",
       "\n",
       "                             transcript_preprocessed  \n",
       "0  talk new lecture TED illusion create TED try r...  \n",
       "1  representation brain brain break left half log...  \n",
       "2  great honor today share Digital Universe creat...  \n",
       "3  passion music technology thing combination thi...  \n",
       "4  use want computer new program programming requ...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ted.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 29158)\n"
     ]
    }
   ],
   "source": [
    "#Importer TfidfVectorizer depuis sklearn.\n",
    "____\n",
    "\n",
    "\n",
    "#Créer un objet TfidfVectorizer. Nommez-le vectorizer.\n",
    "____\n",
    "\n",
    "\n",
    "#Générer tfidf_matrix pour ted en utilisant la méthode fit_transform ()\n",
    "tfidf_matrix = ____\n",
    "\n",
    "\n",
    "# afficher la taille de tfidf_matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans cet exercice, nous allons apprendre à calculer le produit scalaire entre deux vecteurs, A = (1, 3) et B = (-2, 2), à l'aide de la bibliothèque numpy. Plus précisément, nous utiliserons la fonction np.dot () pour calculer le produit scalaire de deux tableaux numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Initialiser A (1,3) et B (-2,2) en tant que tableaux numpy en utilisant np.array ()\n",
    "\n",
    "\n",
    "#Calculer le produit scalaire en utilisant np.dot () et en passant A et B comme arguments.\n",
    "dot_prod = ____\n",
    "\n",
    "\n",
    "# Print dot_prod\n",
    "print(dot_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exercice, vous allez travailler sur la liste ci-dessous. Vous devez calculer la matrice de similarité cosinus qui contient le score de similarité cosinus par paire pour chaque paire de phrases (vectorisée à l'aide de tf-idf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste =['The lion is the king of the jungle',\n",
    " 'Lions have lifespans of a decade',\n",
    " 'The lion is an endangered species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.09040303 0.44446827]\n",
      " [0.09040303 1.         0.        ]\n",
      " [0.44446827 0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Importer cosine_similarity\n",
    "____\n",
    "\n",
    "\n",
    "#Initialiser une instance de TfidfVectorizer. Nommer-le tfidf_vectorizer.\n",
    "\n",
    "____\n",
    "\n",
    "\n",
    "#En utilisant fit_transform (), générer les vecteurs tf-idf pour corpus. Nommer-le tfidf_matrix.\n",
    "\n",
    "\n",
    "tfidf_matrix_list = ____\n",
    "\n",
    "\n",
    "\n",
    "#Utiliser cosine_similarity () et passez tfidf_matrix pour calculer la matrice de similarité cosinus cosine_sim.\n",
    "\n",
    "\n",
    "cosine_sim = ____\n",
    "\n",
    "\n",
    "#afficher la matrice\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voir la vidéo 2 du chapitre 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dans cet exercice, on va utiliser tfidf_matrix du début du chapitre . Votre tâche consiste à générer la matrice de similarité cosinus pour ces vecteurs d'abord en utilisant cosinus_similarity, puis en utilisant linear_kernel.\n",
    "\n",
    "Nous comparerons ensuite les temps de calcul des deux fonctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.09040303 0.44446827]\n",
      " [0.09040303 1.         0.        ]\n",
      " [0.44446827 0.         1.        ]]\n",
      "Time taken: 0.0010004043579101562 seconds\n"
     ]
    }
   ],
   "source": [
    "# enregistrer le temps\n",
    "start = time.time()\n",
    "\n",
    "# calculer cosine similarity matrix\n",
    "cosine_sim = ____\n",
    "\n",
    "\n",
    "# afficher la cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "# afficher le temps de calcul\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.09040303 0.44446827]\n",
      " [0.09040303 1.         0.        ]\n",
      " [0.44446827 0.         1.        ]]\n",
      "Time taken: 0.0010004043579101562 seconds\n"
     ]
    }
   ],
   "source": [
    "# Importer linear_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# enregistrer le temps\n",
    "start = time.time()\n",
    "\n",
    "# calculer cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# afficher la cosine similarity matrix\n",
    "print(cosine_sim)\n",
    "\n",
    "#  afficher le temps de calcul\n",
    "print(\"Time taken: %s seconds\" %(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "La fonction de recommandations\n",
    "\n",
    "Dans cet exercice, nous allons créer une fonction de recommandations get_recommendations (). Elle prendra comme arguments un titre, une matrice de similarité cosinus, un titre de film et un mappage d'index et génèrera une liste de 10 titres les plus similaires au titre original (à l'exclusion du titre lui-même).\n",
    "\n",
    "On utilise le dataframe movies pour les index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generer le  mapping entre titles and index\n",
    "indices = pd.Series(movies.index, index=movies['title']).drop_duplicates()\n",
    "\n",
    "# définisser la fonction get_recommendations(title, cosine_sim, indices)\n",
    "def get_recommendations(title, cosine_sim, indices):\n",
    "    #Obtenir l'index du film qui correspond au titre à l'aide de la clé de titre des index.\n",
    "    ____\n",
    "    # trier les films à partir des \"similarity scores\"\n",
    "    ____\n",
    "    \n",
    "    # obtenir les scores des 10 films les plus proches\n",
    "    ____\n",
    "    \n",
    "    # obtenir les indices\n",
    "    \n",
    "    ____\n",
    "    \n",
    "    # renvoyer le top 10 des films similaires\n",
    "    \n",
    "    return ____\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exercice, nous allons créer un moteur de recommandation qui suggère des films basés sur la similitude des \"overviews\". \n",
    "Les overviews des films sont dans le dataframe movies chargé précédemment.\n",
    "\n",
    "Nous testerons le moteur avec  \"The Dark Knight Rises\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5401            The Stepford Wives\n",
      "994                      Braindead\n",
      "3795                   On the Line\n",
      "5983                        3-Iron\n",
      "8517                Particle Fever\n",
      "3331    Elvis That's the Way It Is\n",
      "5130                      Dogville\n",
      "8123                  End of Watch\n",
      "8666                   The Captive\n",
      "4132      She Wore a Yellow Ribbon\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#on isole des overviews\n",
    "movie_plots = movies[movies.index == indices][\"overview\"].dropna()\n",
    "\n",
    "# Initialiser le TfidfVectorizer en enlevant les stop_wordds\n",
    "tfidf = ____\n",
    "\n",
    "# constuire the TF-IDF matrix\n",
    "tfidf_matrix = ____\n",
    "\n",
    "# générer la cosine similarity matrix\n",
    "cosine_sim = ____\n",
    " \n",
    "# générer des recommendations \n",
    "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
